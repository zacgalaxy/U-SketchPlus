{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zaccu\\OneDrive\\Documents\\GitHub\\U-SketchPlus\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\zaccu\\OneDrive\\Documents\\GitHub\\U-SketchPlus\\.venv\\lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "# Hugging Face Hub import\n",
    "\n",
    "# Diffusers-specific imports\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "# Custom modules\n",
    "\n",
    "from models import UNETLatentEdgePredictor, SketchSimplificationNetwork\n",
    "from pipeline import SketchGuidedText2Image\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sketch Simplifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaccu\\AppData\\Local\\Temp\\ipykernel_5044\\1103916148.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sketch_simplifier.load_state_dict(torch.load(\"models-checkpoints/model_gan.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SketchSimplificationNetwork(\n",
       "  (0): Conv2d(1, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU()\n",
       "  (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): ReLU()\n",
       "  (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (7): ReLU()\n",
       "  (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): ReLU()\n",
       "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU()\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (13): ReLU()\n",
       "  (14): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU()\n",
       "  (16): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU()\n",
       "  (18): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (19): ReLU()\n",
       "  (20): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (21): ReLU()\n",
       "  (22): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (23): ReLU()\n",
       "  (24): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU()\n",
       "  (26): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU()\n",
       "  (28): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (29): ReLU()\n",
       "  (30): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU()\n",
       "  (32): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU()\n",
       "  (34): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (35): ReLU()\n",
       "  (36): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (37): ReLU()\n",
       "  (38): Conv2d(128, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (39): ReLU()\n",
       "  (40): ConvTranspose2d(48, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (41): ReLU()\n",
       "  (42): Conv2d(48, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (43): ReLU()\n",
       "  (44): Conv2d(24, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (45): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure and load sketch simplification network \n",
    "\n",
    "sketch_simplifier = SketchSimplificationNetwork().to(device)\n",
    "sketch_simplifier.load_state_dict(torch.load(\"models-checkpoints/model_gan.pth\"))\n",
    "\n",
    "sketch_simplifier.eval()\n",
    "sketch_simplifier.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Stable Diffusian Model and schdueler for Infernce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stable Diffusion Pipeline\n",
    "stable_diffusion_1_5 = \"benjamin-paine/stable-diffusion-v1-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'use_auth_token': True} are not expected by StableDiffusionPipeline and will be ignored.\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"benjamin-paine/stable-diffusion-v1-5\", use_auth_token=True)\n",
    "pipeline.save_pretrained(\"./stable-diffusion-v1-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  33%|███▎      | 2/6 [00:01<00:03,  1.26it/s]c:\\Users\\zaccu\\OneDrive\\Documents\\GitHub\\U-SketchPlus\\.venv\\lib\\site-packages\\transformers\\models\\clip\\feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:10<00:00,  1.76s/it]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stable_diffusion=StableDiffusionPipeline.from_pretrained(\n",
    "    stable_diffusion_1_5,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None  # Skip the safety checker if it's not required\n",
    ")\n",
    "vae = stable_diffusion.vae.to(device)\n",
    "unet = stable_diffusion.unet.to(device)\n",
    "tokenizer = stable_diffusion.tokenizer\n",
    "text_encoder = stable_diffusion.text_encoder.to(device)\n",
    "\n",
    "vae.eval()\n",
    "unet.eval()\n",
    "text_encoder.eval()\n",
    "\n",
    "text_encoder.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "# Set Scheduler\n",
    "noise_scheduler = DDIMScheduler(\n",
    "        beta_start = 0.00085,\n",
    "        beta_end = 0.012,\n",
    "        beta_schedule = \"scaled_linear\",\n",
    "        num_train_timesteps = 1000,\n",
    "        clip_sample = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet Pipeline and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaccu\\AppData\\Local\\Temp\\ipykernel_5044\\846375469.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"models-checkpoints/unet_latent_edge_predictor_checkpoint.pt\", map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNETLatentEdgePredictor(\n",
       "  (e1): encoder_block(\n",
       "    (conv): convolutional_block(\n",
       "      (conv1): Conv2d(9320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (e2): encoder_block(\n",
       "    (conv): convolutional_block(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (e3): encoder_block(\n",
       "    (conv): convolutional_block(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (e4): encoder_block(\n",
       "    (conv): convolutional_block(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (b): convolutional_block(\n",
       "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (d1): decoder_block(\n",
       "    (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): convolutional_block(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (d2): decoder_block(\n",
       "    (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): convolutional_block(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (d3): decoder_block(\n",
       "    (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): convolutional_block(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (d4): decoder_block(\n",
       "    (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): convolutional_block(\n",
       "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (outputs): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load U-Net latent edge predictor\n",
    "checkpoint = torch.load(\"models-checkpoints/unet_latent_edge_predictor_checkpoint.pt\", map_location=torch.device('cpu'))\n",
    "LEP_UNET = UNETLatentEdgePredictor(9320, 4, 9).to(device)\n",
    "LEP_UNET.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "LEP_UNET.eval()\n",
    "LEP_UNET.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply LoRA to the U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA to the U-Net\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank of LoRA matrix\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\"],  # Target attention layers\n",
    "    lora_dropout=0  # Dropout\n",
    ")\n",
    "\n",
    "unet = get_peft_model(unet, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations\n",
    "image_size = 512\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class SketchImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, image_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = os.path.join(root_dir, \"photos\")\n",
    "        self.sketch_dir = os.path.join(root_dir, \"sketch\")\n",
    "        # Collect all image filenames with common image extensions.\n",
    "        self.image_filenames = [f for f in os.listdir(self.image_dir) if f.endswith((\".jpg\", \".png\"))]\n",
    "        self.image_transform = image_transform  # transform for photos only\n",
    "\n",
    "    def clean_filename(self, filename):\n",
    "        \"\"\"\n",
    "        Cleans the filename for use as a text prompt.\n",
    "          - Removes file extension,\n",
    "          - Splits on \"-\" and uses the first part,\n",
    "          - Replaces underscores with spaces.\n",
    "        \"\"\"\n",
    "        name = os.path.splitext(filename)[0]\n",
    "        name = name.split(\"-\")[0]\n",
    "        name = name.replace(\"_\", \" \")\n",
    "        return name.strip()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the photo image and convert to RGB.\n",
    "        image_name = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)  # Expecting a tensor in [0,1]\n",
    "\n",
    "        # For sketches, we want to keep them as PIL images so that our trainer’s image_to_latents can use numpy.\n",
    "        # We assume each photo has a corresponding folder in the \"sketch\" directory.\n",
    "        sketch_folder = os.path.join(self.sketch_dir, os.path.splitext(image_name + \".jpg\")[0])\n",
    "        if not os.path.exists(sketch_folder):\n",
    "            raise FileNotFoundError(f\"Sketch folder not found: {sketch_folder}\")\n",
    "        sketch_files = [f for f in os.listdir(sketch_folder) if f.endswith((\".jpg\", \".png\"))]\n",
    "        if not sketch_files:\n",
    "            raise ValueError(f\"No sketches found for {image_name}\")\n",
    "        sketch_path = os.path.join(sketch_folder, sketch_files[0])  # Use the first sketch\n",
    "        sketch = Image.open(sketch_path).convert(\"RGB\")  # Keep sketch as PIL image and convert them to grey scale\n",
    "\n",
    "        # Clean the filename to generate a text prompt.\n",
    "        text_prompt = self.clean_filename(image_name)\n",
    "\n",
    "        return {\"image\": image, \"sketch\": sketch, \"text_prompt\": text_prompt, \"filename\": image_name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset and dataloader\n",
    "photo_transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "def custom_collate_fn(batch):\n",
    "    # Batch is a list of dictionaries.\n",
    "    images = torch.stack([item[\"image\"] for item in batch])  # stack photo tensors\n",
    "    text_prompts = [item[\"text_prompt\"] for item in batch]    # leave as list of strings\n",
    "    filenames = [item[\"filename\"] for item in batch]          # leave as list\n",
    "    sketches = [item[\"sketch\"] for item in batch]             # leave sketches as a list of PIL images\n",
    "    return {\"image\": images, \"sketch\": sketches, \"text_prompt\": text_prompts, \"filename\": filenames}\n",
    "\n",
    "\n",
    "# Initialize the dataset.\n",
    "dataset = SketchImageDataset(root_dir=\"Lego_256x256\", image_transform=photo_transform)\n",
    "\n",
    "# Create the DataLoader.\n",
    "train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define Mean Squared Error (MSE) loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Function to compute loss\n",
    "def compute_loss(pred, target):\n",
    "    return criterion(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise Scheduler\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    beta_start=0.00085, beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    num_train_timesteps=1000, clip_sample=False\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(unet.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_embeddings(text):\n",
    "    \"\"\"\n",
    "    Generates text embeddings using the CLIP text encoder.\n",
    "    \"\"\"\n",
    "    tokenized_text = tokenizer(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        max_length=77,  # Standard max length for CLIP\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(tokenized_text.input_ids)[0]  # No `.half()` here\n",
    "\n",
    "    return text_embeddings.float()  # ✅ Convert to float32 to match U-Net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pipeline\n",
    "import TrainingPipeline\n",
    "importlib.reload(TrainingPipeline)\n",
    "from TrainingPipeline import SketchGuidedText2ImageTrainer  # Import again\n",
    "\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SketchGuidedText2ImageTrainer(\n",
    "    stable_diffusion_pipeline=stable_diffusion,\n",
    "    unet=unet,\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder,\n",
    "    lep_unet=LEP_UNET,\n",
    "    scheduler=noise_scheduler,\n",
    "    tokenizer=tokenizer,\n",
    "    sketch_simplifier=sketch_simplifier,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 8.00 GiB\n"
     ]
    }
   ],
   "source": [
    "props = torch.cuda.get_device_properties(0)\n",
    "print(f\"Total GPU Memory: {props.total_memory / (1024**3):.2f} GiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/11 [00:00<?, ?it/s]c:\\Users\\zaccu\\OneDrive\\Documents\\GitHub\\U-SketchPlus\\.venv\\lib\\site-packages\\diffusers\\pipelines\\stable_diffusion\\pipeline_stable_diffusion.py:303: FutureWarning: `_encode_prompt()` is deprecated and it will be removed in a future version. Use `encode_prompt()` instead. Also, be aware that the output format changed from a concatenated tensor to a tuple.\n",
      "  deprecate(\"_encode_prompt()\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "Denoising:   0%|          | 0/50 [00:52<?, ?it/s]\n",
      "Epoch 1/5:   0%|          | 0/11 [00:54<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.14 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.26 GiB is allocated by PyTorch, and 114.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     text_prompts \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Run training step\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msketches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zaccu\\OneDrive\\Documents\\GitHub\\U-SketchPlus\\TrainingPipeline.py:189\u001b[0m, in \u001b[0;36mSketchGuidedText2ImageTrainer.train_step\u001b[1;34m(self, images, sketches, text_prompts, optimizer, noise_scheduler, num_inference_timesteps, classifier_guidance_strength, sketch_guidance_strength, guidance_steps_perc)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gradient:\n\u001b[0;32m    187\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(result \u001b[38;5;241m-\u001b[39m encoded_edge_maps)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m--> 189\u001b[0m     _, grad \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimilarity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43munet_input\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    191\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(latents_old \u001b[38;5;241m-\u001b[39m latents)\u001b[38;5;241m/\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(grad))\u001b[38;5;241m*\u001b[39msketch_guidance_strength\n\u001b[0;32m    193\u001b[0m     latents \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m alpha\u001b[38;5;241m*\u001b[39mgrad\n",
      "File \u001b[1;32mc:\\Users\\zaccu\\OneDrive\\Documents\\GitHub\\U-SketchPlus\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    493\u001b[0m         grad_outputs_\n\u001b[0;32m    494\u001b[0m     )\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    509\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\zaccu\\OneDrive\\Documents\\GitHub\\U-SketchPlus\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.14 GiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 13.26 GiB is allocated by PyTorch, and 114.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Define optimizer\n",
    "optimizer = optim.AdamW(unet.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        sketches= batch[\"sketch\"]\n",
    "        text_prompts = batch[\"text_prompt\"]\n",
    "\n",
    "        # Run training step\n",
    "        loss = trainer.train_step(images, sketches, text_prompts, optimizer, noise_scheduler)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test  Model Interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fine-tuned model\n",
    "unet.save_pretrained(\"fine_tuned_unet\")\n",
    "\n",
    "# Download the model in Jupyter Notebook\n",
    "import shutil\n",
    "shutil.make_archive(\"fine_tuned_unet\", 'zip', \"fine_tuned_unet\")\n",
    "\n",
    "# To download it locally\n",
    "from IPython.display import FileLink\n",
    "FileLink(\"fine_tuned_unet.zip\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Text-guided Text-to-Image synthesis pipeline\n",
    "\n",
    "# Reinitialize the pipeline with the fine-tuned model\n",
    "pipeline = SketchGuidedText2Image(\n",
    "    stable_diffusion_pipeline=stable_diffusion, \n",
    "    unet=unet, vae=vae, \n",
    "    text_encoder=text_encoder, \n",
    "    lep_unet=LEP_UNET, scheduler=noise_scheduler, \n",
    "    tokenizer=tokenizer,\n",
    "    sketch_simplifier=sketch_simplifier,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_maps = [Image.open(\"example-sketches/home.jpg\")]\n",
    "seed = 1000\n",
    "\n",
    "inverse_diffusion = pipeline.Inference(\n",
    "    prompt=[\" Snail in its Shell in the street with many cars \"],\n",
    "    num_images_per_prompt=1,\n",
    "    edge_maps=edge_maps,\n",
    "    negative_prompt=None,\n",
    "    num_inference_timesteps=50,\n",
    "    classifier_guidance_strength=8,\n",
    "    sketch_guidance_strength=1.6,\n",
    "    seed=seed,\n",
    "    simplify_edge_maps=True,\n",
    "    guidance_steps_perc=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge_map, image in zip(edge_maps, inverse_diffusion[\"generated_image\"]):\n",
    "    fig, axs = plt.subplots(1, 2, figsize = (10, 5))\n",
    "    axs[0].imshow(edge_map)\n",
    "    axs[1].imshow(image)\n",
    "    axs[0].axis(\"off\")\n",
    "    axs[1].axis(\"off\")\n",
    "    axs[0].set_title(\"Input Sketch\")\n",
    "    axs[1].set_title(\"Synthesized Image\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
