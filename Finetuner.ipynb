{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zaccu\\OneDrive\\Documents\\GitHub\\U-SketchPlus\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "Torchvision version: 0.20.1+cu121\n",
      "Diffusers version: 0.31.0\n",
      "Einops version: 0.8.1\n",
      "Safetensors version: 0.5.3\n",
      "Albumentations version: 2.0.4\n",
      "Transformers version: 4.30.0\n",
      "Matplotlib version: 3.10.0\n",
      "NumPy version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import diffusers\n",
    "import einops\n",
    "import safetensors\n",
    "import albumentations\n",
    "import transformers\n",
    "import matplotlib\n",
    "import numpy\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "# Hugging Face Hub import\n",
    "\n",
    "# Diffusers-specific imports\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "\n",
    "# Custom modules\n",
    "from models import UNETLatentEdgePredictor, SketchSimplificationNetwork\n",
    "from pipeline import SketchGuidedText2Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sketch Simplifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure and load sketch simplification network \n",
    "\n",
    "sketch_simplifier = SketchSimplificationNetwork().to(device)\n",
    "sketch_simplifier.load_state_dict(torch.load(\"models-checkpoints/model_gan.pth\"))\n",
    "\n",
    "sketch_simplifier.eval()\n",
    "sketch_simplifier.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Stable Diffusian Model and schdueler for Infernce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Stable Diffusion Pipeline\n",
    "stable_diffusion_1_5 = \"benjamin-paine/stable-diffusion-v1-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stable_diffusion=StableDiffusionPipeline.from_pretrained(\n",
    "    stable_diffusion_1_5,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None  # Skip the safety checker if it's not required\n",
    ")\n",
    "vae = stable_diffusion.vae.to(device)\n",
    "unet = stable_diffusion.unet.to(device)\n",
    "tokenizer = stable_diffusion.tokenizer\n",
    "text_encoder = stable_diffusion.text_encoder.to(device) \n",
    "\n",
    "vae.eval()\n",
    "unet.eval()\n",
    "text_encoder.eval()\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "# Set Scheduler\n",
    "noise_scheduler = DDIMScheduler(\n",
    "        beta_start = 0.00085,\n",
    "        beta_end = 0.012,\n",
    "        beta_schedule = \"scaled_linear\",\n",
    "        num_train_timesteps = 1000,\n",
    "        clip_sample = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet Pipeline and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load U-Net latent edge predictor\n",
    "checkpoint = torch.load(\"models-checkpoints/unet_latent_edge_predictor_checkpoint.pt\",map_location=torch.device('cpu'))\n",
    "\n",
    "LEP_UNET = UNETLatentEdgePredictor(9320, 4, 9).to(device)\n",
    "LEP_UNET.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "LEP_UNET.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to convert images to VAE latent embeddings\n",
    "def encode_image_to_latent(image_path, vae, device):\n",
    "    image = Image.open(image_path).convert(\"L\")  # Convert to grayscale\n",
    "    transform = transforms.ToTensor()\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        latent = vae.encode(image_tensor).latent_dist.sample()  # Get VAE latent\n",
    "    return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to dataset\n",
    "sketch_dir = \"Lego 256x256/sketches\"\n",
    "image_dir = \"Lego 256x256/images\"\n",
    "\n",
    "# Get list of sketches\n",
    "sketch_files = sorted(os.listdir(sketch_dir))\n",
    "\n",
    "# Store latents for training\n",
    "sketch_latents = []\n",
    "lego_latents = []\n",
    "time_embeddings = []\n",
    "\n",
    "for file in sketch_files:\n",
    "    sketch_path = os.path.join(sketch_dir, file)\n",
    "    image_path = os.path.join(image_dir, file)  # Assume matching filenames\n",
    "\n",
    "    # Encode sketch and Lego image into latent space\n",
    "    sketch_latent = encode_image_to_latent(sketch_path, vae, device)\n",
    "    lego_latent = encode_image_to_latent(image_path, vae, device)\n",
    "\n",
    "    # Generate a random time embedding (for diffusion guidance)\n",
    "    time_embedding = torch.rand(1, sketch_latent.shape[-1]).to(device)\n",
    "\n",
    "    # Store latents\n",
    "    sketch_latents.append(sketch_latent)\n",
    "    lego_latents.append(lego_latent)\n",
    "    time_embeddings.append(time_embedding)\n",
    "\n",
    "# Convert lists to tensors\n",
    "sketch_latents = torch.stack(sketch_latents)\n",
    "lego_latents = torch.stack(lego_latents)\n",
    "time_embeddings = torch.stack(time_embeddings)\n",
    "\n",
    "print(f\"Loaded {len(sketch_latents)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define Mean Squared Error (MSE) loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Function to compute loss\n",
    "def compute_loss(pred, target):\n",
    "    return criterion(pred, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set training parameters\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-4\n",
    "batch_size = 8\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.AdamW(LEP_UNET.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i in range(0, len(sketch_latents), batch_size):\n",
    "        # Get batch\n",
    "        batch_sketch = sketch_latents[i:i+batch_size].to(device)\n",
    "        batch_lego = lego_latents[i:i+batch_size].to(device)\n",
    "        batch_time = time_embeddings[i:i+batch_size].to(device)\n",
    "\n",
    "        # Combine sketch latents with time embeddings\n",
    "        input_embedding = torch.cat([batch_sketch.flatten(start_dim=1), batch_time.flatten(start_dim=1)], dim=1)\n",
    "\n",
    "        # Forward pass through LEP UNet\n",
    "        optimizer.zero_grad()\n",
    "        output_latent = LEP_UNET(input_embedding)  # Predict latent\n",
    "\n",
    "        # Compute loss\n",
    "        loss = compute_loss(output_latent, batch_lego)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Print loss for epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(sketch_latents):.4f}\")\n",
    "\n",
    "# Save fine-tuned model\n",
    "torch.save(LEP_UNET.state_dict(), \"LEP_UNET_finetuned.pth\")\n",
    "print(\"Model fine-tuned and saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test  Model Interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEP_UNET.load_state_dict(torch.load(\"LEP_UNET_finetuned.pth\"))\n",
    "LEP_UNET.eval()\n",
    "\n",
    "print(\"Fine-tuned model loaded for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Text-guided Text-to-Image synthesis pipeline\n",
    "\n",
    "pipeline = SketchGuidedText2Image(stable_diffusion_pipeline = stable_diffusion, \n",
    "                                  unet = unet, vae = vae, \n",
    "                                  text_encoder = text_encoder, \n",
    "                                  lep_unet = LEP_UNET, scheduler = noise_scheduler, \n",
    "                                  tokenizer = tokenizer,\n",
    "                                  sketch_simplifier = sketch_simplifier,\n",
    "                                  device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_maps = [Image.open(\"example-sketches/home.jpg\")]\n",
    "seed = 1000\n",
    "\n",
    "inverse_diffusion = pipeline.Inference(\n",
    "    prompt=[\" Snail in its Shell in the street with many cars \"],\n",
    "    num_images_per_prompt=1,\n",
    "    edge_maps=edge_maps,\n",
    "    negative_prompt=None,\n",
    "    num_inference_timesteps=50,\n",
    "    classifier_guidance_strength=8,\n",
    "    sketch_guidance_strength=1.6,\n",
    "    seed=seed,\n",
    "    simplify_edge_maps=True,\n",
    "    guidance_steps_perc=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge_map, image in zip(edge_maps, inverse_diffusion[\"generated_image\"]):\n",
    "    fig, axs = plt.subplots(1, 2, figsize = (10, 5))\n",
    "    axs[0].imshow(edge_map)\n",
    "    axs[1].imshow(image)\n",
    "    axs[0].axis(\"off\")\n",
    "    axs[1].axis(\"off\")\n",
    "    axs[0].set_title(\"Input Sketch\")\n",
    "    axs[1].set_title(\"Synthesized Image\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
